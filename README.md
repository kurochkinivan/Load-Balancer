# Load Balancer - балансировщик нагрузки 

## Инструкция по запуску

```bash
## Если есть утилита task:
task start
```

```bash
## Если нет утилиты task:

## 1) запуск фейковых серверов
go run cmd/servers/servers.go --servers=10 --start_port=8100 &

## 2) запуск postgresql в докере
docker compose up -d

## 3) запуск балансировщика нагрузки
go run cmd/loadBalancer/loadBalancer.go --path=config/config.yaml
```

**Swagger** для API клиентов.

Сваггер лежит в docs/swagger.yaml (сорян, сегодня без go-swagger)

**Clients API**:

```bash
## Создать клиента:
curl -X POST http://localhost:8080/v1/api/clients/ \
  -H "Content-Type: application/json" \
  -d '{"ip_address": "127.0.0.1", "capacity": 100, "rate_per_second": 10}'

## Изменить клиента по его IP: 
curl -X PUT http://localhost:8080/v1/api/clients/127.0.0.1 \
   -H "Content-Type: application/json" \
   -d '{"capacity": 200, "rate_per_second": 50}'

## Получить всех клиентов:
curl -X GET http://localhost:8080/v1/api/clients/

## Удалить клиента по его IP: 
curl -X DELETE http://localhost:8080/v1/api/clients/127.0.0.1
```

**Запуск тестов:**

**ОЧЕНЬ ВАЖНО**: я сделал два варианта поведения для middleware.RateLimitingMiddleware. 
1) Если передается clientCreator, то при запросе от неизвестного клиента создается дефолтный клиент в БД и запрос проходит дальше. В коде сейчас так и сделано.
2) Если передается nil в качестве clientCreator, то от всех неизвестных клиентов будут блокироваться запросы (кроме запросов на создание клиентов в БД, т.е. кроме /v1/api/clients/).

Поведение можно изменить в файле internal/app/httpapp при инициализации RateLimitingMiddleware.

```bash
## Легкий тест:
ab -n 5000 -c 100 http://localhost:8080/

## Тяжелый тест
ab -n 20000 -c 1000 http://localhost:8080/

## Один запрос (можно и не /test, это в качестве примера):
curl localhost:8080/test
```

## Ответы на вопросы:

1. Опишите самую интересную задачу в программировании, которую вам приходилось решать?

   Я могу с уверенностью утверждать, что LoadBalancer является одной из самых интересных задач, которую я решал в принципе) Очень приятно было не тупо описывать CRUD, а делать балансировку нагрузки, лимит запросов, работать с горутинами - наконец получилось полноценно применить свои знания)

   Если говорить о других задачах, мне запомнилось, как я реализовывал Маджонг-Коннект. Я реализовал алгоритм поиска в ширину в матрице, чтобы решить задачу о нахождении пути между вершинами с ровно двумя поворотами. Это было интересно, так как пригодились алгоритмические знания.

   Также мне нравится разбираться в новых технологиях. К примеру, сейчас я разрабатываю приложение Meet - там я использую gRPC, AWS (с Yandex Cloud) и redis. 

2. Расскажите о своем самом большом факапе? Что вы предприняли для решения проблемы?

   Когда я и моя команда учавствовали в хакатоне и подошел момент соединения фронта и бэка, у нас начала выпадать ошибка политики CORS. Сначала я малек запаниковал, так как до презентации оставалось 2 часа. Но затем я успокоился, организовал ребят, чтобы они делали презентацию и дорабатывали функционал, а сам сел разбираться с этой ошибкой. В итоге в этом хакатоне мы победили)

3. Каковы ваши ожидания от участия в буткемпе?

   Я очень хочу поработать в команде профессионалов, изучить лучшие практики, углубить свои знания. А вообще хочется проявить себя и попасть в штат после окончания стажировки) 

## Описание проекта:

#### При запуске проекта:

1) Создаются фейковые сервера - по дефолту 10 штук, но это можно изменить с помощью флага, равно как и начальный порт для серверов;
2) Запускается postgresql в докере для сохранения конфигураций клиентов;
3) Запускается сам балансировщик нагрузки.

---

#### Что происходит внутри балансировщика нагрузки?

Одновременно функционируют 5 горутин:

-  основной поток программы, который ожидает сигнала от операционной системы (SIGTERM, SIGINT) для Graceful Shutdown;
-  клиент для postgresql (pgapp)
-  http-сервер (httpapp)
   -  health check всех бэкендов раз в interval времени (reverseProxy.StartHealthChecks);
   -  пополнение токенов для клиентов раз в секунду (clientsCache.StartTokenRefiller).
   -  мультиплексор запросов (взял роутер джулиана шмидта, чтобы удобнее с параметрами работать)

## Реализованный функционал:

1) Reverse-proxy;
2) Алгоритм распределения запросов по бэкендам;
3) Логгирование;
4) Чтение конфигурационного файла, независимого от кода;
5) Корректная работа в условиях конкурентных вызовов;
6) Обработка ошибок (отправка единой ошибки httperror.HTTPError);
7) Реализация TokenBucket;
8) Хранение конфигураций клиентов в БД;
9) API для создания, удаления и получения конфига клиентов;
10) Graceful Shutdown;
11) Здоровье бэкендов (Health Checks);
12) Поддержка нескольких алгоритмов распределения;

## Конкурентность

1) atomic - доступность/недоступность бэкенда;
2) atomic - количество токенов у клиента;
3) подсчитывающий семафор - ограничение параллельности для healthCheck;
4) sync.Map - реализация локального кэша.

## Архитектура:

В коде я использовал подход с **трехслойной архитектурой**:
1) Контроллер - controller/
2) Сервсиный слой - usecase/
3) Слой доступа к данным - usecase/storage/
4) Сущности приложения - entity/

Интерфейсы располагал **по месту использования**. 
Использовал **инъекции зависимостей** для удаления зависимостей слоев друг от друга.
